# Sign-Language-Detection-python

Sign language detector with Python, OpenCV and Mediapipe !

# Abstract
This project discusses the development of a robust sign language recognition system using Python, OpenCV, and a Convolutional Neural Network (CNN) based on the Pre-Trained SSD MobileNet V2 architecture. Trained on a custom dataset, the system achieves an accuracy of 70-80% in recognizing signs in uncontrolled environments with varying light conditions, enhancing communication for the deaf community and aiding sign language learners

### Objectives

1. Develop an accurate real-time Sign Language Recognition System.
2. Create a user-friendly interface for educators and students.
3. Improve system accuracy through machine learning.
4. Ensure integration with existing educational platforms.
5. Validate effectiveness through testing.
6. Provide training for educators.
7. Gather feedback for continuous improvement.
8. Promote adoption for inclusive education.
9. Address ethical considerations.
10. Establish long-term sustainability.

### Applications

The platform's applications include inclusive learning environments, accessible education for remote areas, personalized support services, enhanced communication in public spaces, skills enhancement for educators and professionals, accessible multimedia materials, inclusion in the workforce, tailored education for unique needs, research advancements, and global inclusivity.

## Data Collection & Analysis

### Data Details

The project involved collecting a diverse dataset of sign language gestures, annotating and labeling the dataset with corresponding meanings, preprocessing and extracting features, training machine learning models, evaluating performance metrics, conducting error analysis, and refining the models iteratively.

### Data Insights
<table>
  <tr>
    <td align="center"><img src="https://github.com/sanika391/Sign-Language-Detection/assets/116996971/9f2351d0-17b6-46b5-ba5b-1a0aece770d5" style="width: 200px;"></td>
    <td align="center"><img src="https://github.com/sanika391/Sign-Language-Detection/assets/116996971/e1492258-553a-4ab5-9c7b-d319a8540f19" alt="Image 2" style="width: 200px;"></td>
    <td align="center"><img src="https://github.com/sanika391/Sign-Language-Detection/assets/116996971/3ce8ec64-f456-469c-b569-68310af249dd" alt="Image 3" style="width: 200px;"></td>
  </tr>
</table>

### Project Goals
1. Develop a sign language recognition model:Utilize Python and OpenCV for image and video processing. Implement a CNN model using the pre-trained SSD MobileNet V2 architecture.

2. Achieve high accuracy:Target recognition accuracy of 70-80% for various sign language gestures.

3. Support diverse environments:Ensure the model works effectively in different lighting and background conditions.

4. Facilitate communication:Help the deaf community by translating sign language into text or speech. Aid learners in practicing and improving their sign language skills.

### Methodology
1. Data Collection: Gather a diverse dataset of sign language gestures, ensuring variability in background, lighting, and hand positions.

2. Data Preprocessing: Perform data augmentation techniques such as rotation, scaling, and flipping to increase the robustness of the model. Normalize the images for consistent input to the neural network.

3. Model Architecture: Use the SSD MobileNet V2 architecture for object detection and feature extraction. Fine-tune the pre-trained model on the collected dataset to adapt it to sign language recognition.

4.Training and Evaluation: Split the dataset into training, validation, and test sets (e.g., 70% training, 15% validation, 15% testing). Train the model using appropriate loss functions and optimizers. Evaluate the model's performance on the test set, aiming for 70 - 80% Accuracy.

### Process Flow:
![Screenshot 2024-07-06 232010](https://github.com/sanika391/Sign-Language-Detection/assets/116996971/d4f6539f-af12-4a5d-9fd5-61c16ebd230a)



